{
  "model_type": "sae",
  "model_name": "gpt2",
  "dataset_name": "EleutherAI/SmolLM2-135M-10B",
  "dataset_split": "train",
  "max_samples": 2000,
  "max_seq_length": 512,
  "expansion_factor": 32,
  "num_latents": 0,
  "k": 32,
  "activation": "topk",
  "normalize_decoder": true,
  "multi_topk": false,
  "skip_connection": false,
  "batch_size": 4,
  "grad_acc_steps": 4,
  "micro_acc_steps": 1,
  "loss_fn": "fvu",
  "optimizer": "adam",
  "lr": null,
  "lr_warmup_steps": 1000,
  "weight_decay": 0.0,
  "auxk_alpha": 0.03125,
  "dead_feature_threshold": 5000000,
  "k_decay_steps": 20000,
  "layers": null,
  "layer_stride": 1,
  "hookpoints": [
    "h.*.attn",
    "h.*.mlp.act"
  ],
  "max_steps": null,
  "init_seeds": [
    42
  ],
  "finetune": false,
  "exclude_tokens": [],
  "distribute_modules": false,
  "save_dir": "./sae_checkpoints",
  "run_name": "custom_hookpoints_attn_mlp",
  "save_every": 1000,
  "save_best": true,
  "log_to_wandb": false,
  "wandb_project": "sae-transcoder-unified",
  "wandb_entity": null,
  "wandb_log_frequency": 100,
  "device": "cuda:1",
  "dtype": "bfloat16"
}