{
  "model_type": "sae",
  "model_name": "HuggingFaceTB/SmolLM2-135M",
  "dataset_name": "EleutherAI/SmolLM2-135M-10B",
  "dataset_split": "train",
  "max_samples": 10000,
  "max_seq_length": 1024,
  "expansion_factor": 64,
  "num_latents": 0,
  "k": 64,
  "activation": "topk",
  "normalize_decoder": true,
  "multi_topk": true,
  "skip_connection": false,
  "batch_size": 8,
  "grad_acc_steps": 4,
  "micro_acc_steps": 1,
  "loss_fn": "fvu",
  "optimizer": "adam",
  "lr": 0.0001,
  "lr_warmup_steps": 2000,
  "weight_decay": 0.0,
  "auxk_alpha": 0.03125,
  "dead_feature_threshold": 5000000,
  "k_decay_steps": 20000,
  "layers": null,
  "layer_stride": 2,
  "hookpoints": null,
  "max_steps": null,
  "init_seeds": [
    42
  ],
  "finetune": false,
  "exclude_tokens": [],
  "distribute_modules": false,
  "save_dir": "./sae_checkpoints",
  "run_name": "advanced_sae_full_config",
  "save_every": 500,
  "save_best": true,
  "log_to_wandb": true,
  "wandb_project": "advanced-sae-training",
  "wandb_entity": null,
  "wandb_log_frequency": 50,
  "device": "cuda:1",
  "dtype": "bfloat16"
}