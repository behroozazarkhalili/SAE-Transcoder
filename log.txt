2025-10-01 15:58:58,398 - INFO - 🚀 Initializing Transcoder Trainer
2025-10-01 15:58:58,399 - INFO - 📋 Configuration: basic_transcoder
2025-10-01 15:58:58,399 - INFO - 🎯 Model: HuggingFaceTB/SmolLM2-135M
2025-10-01 15:58:58,399 - INFO - 📊 Dataset: EleutherAI/SmolLM2-135M-10B
2025-10-01 15:58:58,399 - INFO - 📥 Loading model and tokenizer...
`torch_dtype` is deprecated! Use `dtype` instead!
2025-10-01 15:58:59,197 - INFO - 📚 Loading and processing dataset...
2025-10-01 15:59:00,824 - INFO - 📋 Limited dataset to 1,000 samples
Map (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Map (num_proc=16):   6%|▋         | 63/1000 [00:00<00:05, 168.58 examples/s]Map (num_proc=16):  32%|███▏      | 315/1000 [00:00<00:00, 781.37 examples/s]Map (num_proc=16):  57%|█████▋    | 566/1000 [00:00<00:00, 1225.02 examples/s]Map (num_proc=16):  75%|███████▌  | 752/1000 [00:00<00:00, 1163.13 examples/s]Map (num_proc=16):  94%|█████████▍| 938/1000 [00:00<00:00, 1327.72 examples/s]Map (num_proc=16): 100%|██████████| 1000/1000 [00:01<00:00, 958.24 examples/s]
2025-10-01 15:59:02,722 - INFO - ✅ Dataset ready: 981 samples
2025-10-01 15:59:02,723 - INFO - 
============================================================
2025-10-01 15:59:02,723 - INFO - TRANSCODER CONFIGURATION
2025-10-01 15:59:02,723 - INFO - ============================================================
2025-10-01 15:59:02,723 - INFO - 🏗️  Architecture:
2025-10-01 15:59:02,723 - INFO -    - Mode: END-TO-END TRANSCODER
2025-10-01 15:59:02,723 - INFO -    - Expansion factor: 32
2025-10-01 15:59:02,723 - INFO -    - Number of latents: auto
2025-10-01 15:59:02,723 - INFO -    - Top-k sparsity: 32
2025-10-01 15:59:02,723 - INFO -    - Activation function: topk
2025-10-01 15:59:02,723 - INFO -    - Normalize decoder: True
2025-10-01 15:59:02,723 - INFO -    - Multi-TopK: False
2025-10-01 15:59:02,723 - INFO -    - Skip connection: False
2025-10-01 15:59:02,723 - INFO - 
2025-10-01 15:59:02,723 - INFO - 🎯 Training:
2025-10-01 15:59:02,723 - INFO -    - Batch size: 8
2025-10-01 15:59:02,723 - INFO -    - Gradient accumulation: 4
2025-10-01 15:59:02,723 - INFO -    - Loss function: fvu (end-to-end)
2025-10-01 15:59:02,723 - INFO -    - Optimizer: adam
2025-10-01 15:59:02,723 - INFO -    - Learning rate: auto
2025-10-01 15:59:02,723 - INFO -    - AuxK alpha: 0.03125
2025-10-01 15:59:02,723 - INFO -    - Target layers: [6, 7, 8]
2025-10-01 15:59:02,723 - INFO - 
2025-10-01 15:59:02,723 - INFO - 💾 Saving:
2025-10-01 15:59:02,723 - INFO -    - Save directory: ./transcoder_checkpoints
2025-10-01 15:59:02,723 - INFO -    - Save every: 2000 steps
2025-10-01 15:59:02,723 - INFO -    - Save best: True
2025-10-01 15:59:02,723 - INFO - ============================================================
2025-10-01 15:59:02,723 - INFO - 📊 Evaluating baseline model performance...
2025-10-01 15:59:03,889 - INFO - 📈 Baseline model loss: 3.0075
2025-10-01 15:59:03,890 - INFO - 🏋️ Creating trainer...
2025-10-01 15:59:03,929 - INFO - 🚀 Starting end-to-end Transcoder training...
Training on modules: ['layers.6', 'layers.7', 'layers.8']
Initializing SAEs with random seed(s) [42]
bitsandbytes 8-bit Adam not available, using torch.optim.Adam
Run `pip install bitsandbytes` for less memory usage.
Learning rate: 1.89e-04
Number of SAE parameters: 63_758_016
Number of model parameters: 134_515_008
Training:   0%|          | 0/122 [00:00<?, ?it/s]2025-10-01 15:59:04,618 - ERROR - ❌ Training failed: CUDA out of memory. Tried to allocate 50.62 GiB. GPU 1 has a total capacity of 23.67 GiB of which 21.19 GiB is free. Including non-PyTorch memory, this process has 2.47 GiB memory in use. Of the allocated memory 1.85 GiB is allocated by PyTorch, and 384.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/behrooz/Projects/SAE-Transcoder/train_transcoder.py", line 390, in <module>
    main()
    ~~~~^^
  File "/home/behrooz/Projects/SAE-Transcoder/train_transcoder.py", line 383, in main
    trained_model = trainer.train()
  File "/home/behrooz/Projects/SAE-Transcoder/train_transcoder.py", line 317, in train
    trainer.fit()
    ~~~~~~~~~~~^^
  File "/home/behrooz/miniconda3/lib/python3.13/site-packages/sparsify/trainer.py", line 502, in fit
    self.model(x)
    ~~~~~~~~~~^^^
  File "/home/behrooz/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/behrooz/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/behrooz/miniconda3/lib/python3.13/site-packages/transformers/utils/generic.py", line 940, in wrapper
    output = func(self, *args, **kwargs)
  File "/home/behrooz/miniconda3/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py", line 459, in forward
    outputs: BaseModelOutputWithPast = self.model(
                                       ~~~~~~~~~~^
        input_ids=input_ids,
        ^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "/home/behrooz/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/behrooz/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/behrooz/miniconda3/lib/python3.13/site-packages/transformers/utils/generic.py", line 1064, in wrapper
    outputs = func(self, *args, **kwargs)
  File "/home/behrooz/miniconda3/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py", line 395, in forward
    hidden_states = decoder_layer(
        hidden_states,
    ...<5 lines>...
        **kwargs,
    )
  File "/home/behrooz/miniconda3/lib/python3.13/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/behrooz/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/home/behrooz/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1879, in _call_impl
    return inner()
  File "/home/behrooz/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1840, in inner
    hook_result = hook(self, args, result)
  File "/home/behrooz/miniconda3/lib/python3.13/site-packages/sparsify/trainer.py", line 447, in hook
    loss.div(acc_steps).backward()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/home/behrooz/miniconda3/lib/python3.13/site-packages/torch/_tensor.py", line 647, in backward
    torch.autograd.backward(
    ~~~~~~~~~~~~~~~~~~~~~~~^
        self, gradient, retain_graph, create_graph, inputs=inputs
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/behrooz/miniconda3/lib/python3.13/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
    ~~~~~~~~~~~~~~~~~~~~^
        tensors,
        ^^^^^^^^
    ...<5 lines>...
        accumulate_grad=True,
        ^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/behrooz/miniconda3/lib/python3.13/site-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        t_outputs, *args, **kwargs
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
    )  # Calls into the C++ engine to run the backward pass
    ^
  File "/home/behrooz/miniconda3/lib/python3.13/site-packages/torch/autograd/function.py", line 311, in apply
    return user_fn(self, *args)
  File "/home/behrooz/miniconda3/lib/python3.13/site-packages/sparsify/fused_encoder.py", line 71, in backward
    contributions = grad_values.unsqueeze(2) * input.unsqueeze(1)
                    ~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.62 GiB. GPU 1 has a total capacity of 23.67 GiB of which 21.19 GiB is free. Including non-PyTorch memory, this process has 2.47 GiB memory in use. Of the allocated memory 1.85 GiB is allocated by PyTorch, and 384.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Training:   0%|          | 0/122 [00:00<?, ?it/s]
