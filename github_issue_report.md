# Bug Report: IndexError in trainer.py - Dimension out of range in mask.flatten()

## Summary
SAE training fails with `IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)` at `sparsify/trainer.py:385` when using default configurations.

## Environment
- **Library**: eai-sparsify v1.2.1 (latest)
- **Python**: 3.13
- **PyTorch**: Latest compatible version
- **CUDA**: Available
- **Models tested**: HuggingFaceTB/SmolLM2-135M, EleutherAI/pythia-160m

## Error Details

### Stack Trace
```
IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)
File "sparsify/trainer.py", line 385, in hook
    mask = mask.flatten(0, 1)
```

### Root Cause Analysis
The issue occurs in the forward hook where `identity_mask` is initialized as an empty tensor:

```python
# Line 300 in trainer.py
identity_mask = torch.empty([], dtype=torch.bool, device=device)
```

When `exclude_tokens` is not configured (default behavior), this empty tensor is used as the mask. The tensor has no dimensions, so calling `flatten(0, 1)` fails because dimension 1 doesn't exist.

## Reproduction Steps

### Minimal Reproduction Case

```python
import torch
from datasets import Dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from sparsify import SaeConfig, Trainer, TrainConfig

# Minimal dataset
input_ids = [[1, 2, 3, 4, 5]]
dataset = Dataset.from_dict({'input_ids': input_ids})
dataset = dataset.with_format('torch', columns=['input_ids'])

# Load model
tokenizer = AutoTokenizer.from_pretrained("EleutherAI/pythia-160m")
model = AutoModelForCausalLM.from_pretrained(
    "EleutherAI/pythia-160m",
    device_map={"": "cuda"},
    torch_dtype=torch.bfloat16,
)

# Minimal config
cfg = TrainConfig(SaeConfig(), batch_size=1)
trainer = Trainer(cfg, dataset, model)

# This will fail with the dimension error
trainer.fit()
```

### Full Documentation Example
Even following the exact documentation example fails:

```python
import torch
from datasets import load_dataset, Dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from sparsify import SaeConfig, Trainer, TrainConfig
from sparsify.data import chunk_and_tokenize

MODEL = "HuggingFaceTB/SmolLM2-135M"
dataset = load_dataset("EleutherAI/SmolLM2-135M-10B", split="train", streaming=True)
dataset = dataset.take(1000)

# Convert streaming dataset for compatibility
dataset = Dataset.from_dict({"text": [item["text"] for item in dataset]})

tokenizer = AutoTokenizer.from_pretrained(MODEL)
tokenized = chunk_and_tokenize(dataset, tokenizer)

gpt = AutoModelForCausalLM.from_pretrained(
    MODEL,
    device_map={"": "cuda"},
    torch_dtype=torch.bfloat16,
)

cfg = TrainConfig(SaeConfig(), batch_size=16)
trainer = Trainer(cfg, tokenized, gpt)

trainer.fit()  # Fails with dimension error
```

## Investigation Results

### Tensor Analysis
- Model outputs have correct shape: `[batch, sequence, hidden]` e.g., `[1, 5, 768]`
- The `identity_mask` tensor is the problem: initialized with shape `[]` (no dimensions)
- Other tensors (`outputs`, `inputs`) flatten correctly

### Tested Configurations
âœ… **All fail with same error**:
- Different models (SmolLM2-135M, pythia-160m)
- Real vs synthetic datasets
- Various batch sizes (1, 4, 16)
- Minimal vs comprehensive configurations
- Both streaming and regular datasets

## Expected Behavior
The `identity_mask` should be properly initialized with the same batch and sequence dimensions as the model outputs, allowing `flatten(0, 1)` to work correctly.

## Proposed Fix
The `identity_mask` initialization should match the dimensions of the actual outputs. The mask gets properly set in the hook at line 408:

```python
identity_mask = torch.ones_like(outputs, dtype=torch.bool)
```

But this happens too late - the mask is used before the hook runs to set its proper dimensions.

## Impact
- **Severity**: Critical - prevents any SAE training with default configurations
- **Scope**: Affects all users trying to use the library as documented
- **Workaround**: None currently available for default usage

## Additional Context
This issue affects the latest version (1.2.1) and appears to be a regression or oversight in the mask initialization logic. The library cannot currently fulfill its primary purpose of training sparse autoencoders.

## Files for Testing
The reproduction cases above are minimal and should consistently reproduce the issue on any system with the required dependencies.